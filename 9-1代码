
import re,requests,codecs,time,random
from lxml import html
 
 
#proxies={"http": "http://136.228.128.6:43117"   # http  型的}
proxies=None
headers = {
    'Host': 'hq.sinajs.cn',
    'Connection': 'close',
    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/85.0.4168.3 Safari/537.36'}
def get_url(page):
    stocknum='002594'
    url = 'http://guba.sina.com.cn/?s=bar&name=sz' + stocknum + '&type=0&page=' + str(page)
    try:
        text=requests.get(url)
        requests.adapters.DEFAULT_RETRIES = 5
        s = requests.session()
        s.keep_alive = False
        text=html.fromstring(text.text)
        urls = text.xpath('//*[@id="blk_list_02"]/table/tbody/tr[not(@class="tit_tr")]/td[3]/a/@href')
    except Exception as e:
        print(e)
        time.sleep(random.random() + random.randint(0, 1))
        urls=''
    return urls
def get_comments(urls):
    for newurl in urls:
        newurl1 = 'http://guba.sina.com.cn' + newurl
        time.sleep(random.random() + random.randint(0, 2))
        try:
            text1=requests.get(newurl1)
            requests.adapters.DEFAULT_RETRIES = 5
            s = requests.session()
            s.keep_alive = False
            text1=html.fromstring(text1.text)
            times1 = text1.xpath('//*[@id="thread"]/div[2]/div[6]/div[1]/span/text()')
            times='!'.join(re.sub(re.compile('发表于| '),'',x)[:10] for x in times1).split('!')
            #times=list(map(lambda x:re.sub(re.compile('发表于| '),'',x)[:10],times))
            comments1 = text1.xpath('//*[@id="thread"]/div[2]/h4/text()')
            comments='!'.join(w.strip() for w in comments1).split('!')
            dic=dict(zip(times,comments))
            save_to_file(dic)
        except:
            print('error!!!!')
            time.sleep(random.random()+random.randint(0, 3))
        #print(dic)
        #if times and comments:
            #dic.append({'time':times,'comment':comments})
    #return dic
def save_to_file(dic):
    if dic:
        #dic=dic
        print(dic)
        #df=pd.DataFrame([dic]).T
        #df.to_excel('eastnoney.xlsx')
        for i,j in dic.items():
            output='{}\t{}\n'.format(i,j)
            f=codecs.open('eastmoney.xls','a+','utf-8')
            f.write(output)
            f.close()
 
for page in range(1, 85):
    print('正在爬取第{}页'.format(page))
    urls=get_url(page)
    dic=get_comments(urls)
